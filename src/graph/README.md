# retrocheck.graph

## Definitions

*Generator*: Code that generates instances of entities.

*Data loader*: Code that loads entity instances into, and removes entity instances from, data stores.

*Data model realization*: Given a data model *m*, which is made of a collection *E* of entities and a collection *C* of constraints on those entities, *m*'s realization is a collection of instances of each entity in *E*, such that all constraints in *C* are satisfied.

## Features

- Represents a data model as a graph, in which entities are nodes, and constraints are edges.
- Associates generators with nodes, and constraint functions with edges.
- Associates a data loader with each node, so that the entity represented by each node can be conveniently loaded into the appropriate data store.
- Creates an instance of a data model according to the generators associated with each node, and the constraints associated with each edge, via data model realization.
- Generates HTML/JS/CSS visualizations of the data model specified by the user, as well as of realizations of that data model created by the retrocheck.graph library.

## Data Model as Graph

A graph is a collection of nodes, edges, and smaller graphs.  Formally, a `Graph` is a collection of `Node<?>`s, `Edge<?, ?>`s, and `Subgraph`s.

Generators are implemented by the user (or by `DefaultGenerator`) as classes which extend the `Generator` interface.

Generators are associated with nodes as arguments to the constructor of `Node<T>`.  The expectation is that a generator that's passed as argument to `Node<T>` must be able to generate instances of type `T`.

A constraint is implemented as a user-specified lambda of type `(U, V) -> V` that is passed to the constructor of `Edge<U, V>`. 

## Probability

Each node and edge in a graph can have a probability assigned to it.  For a node *n*, this is the probability that an instance of the entity represented by *n* will appear in a data model realization.  For an edge *e*, this is the probability that *e*'s constraint will be satisfied in a data model realization.

## Uniqueness

Some data models require uniqueness across entity values, e.g. when modeling relational database tables that have uniqueness constraints on keys.  The generators associated with each node in a graph can be made to generate only unique values via the `Unique` class, which guarantees uniqueness of the values generated by a `Generator` by keeping track of all values generated so far.  Uniqueness can be guaranteed across multiple `Generator` instances via `Unique.unify()`.

## Edge Sets

Data model realization supports the notion of edge sets.  For example, given three edges *D*, *E*, and *F*, assigning { *D*, *E*, *F* } to an edge set *S* means that only one edge will be chosen from *S* during data model realization.  The probabilities of all edges in a set must sum to 100.

## Data Model Realization

Data model realization is a process which takes a data model and returns an instance of it.  More formally, data model realization is a transformation which turns a `Graph` into a `Workflow`.  In this documentation, "data model realization" can refer either to the aforementioned transformation, or to the resulting `Workflow` instance.

The `Workflow` that is created by data model realization has to meet the following requirements:

1. Nodes and edges are chosen according to their assigned probabilities (via `BucketOfDestiny`).
2. Entity instances are generated for each chosen node, and constraints are satisfied for each chosen edge.
3. Entity instances are ordered in the resulting `Workflow` such that they can be loaded into data stores without violating any constraints (e.g. foreign key constraints), and vice-versa for unloading.

Data model realization is implemented as a sequence of depth-first searches on the `Graph` representing the data model, along with a few intermediate steps:

1. Source nodes are found (nodes without in-edges).  Each DFS begins from these nodes.
2. DFS to find the level of each node in the resulting depth-first tree.  These levels are used to determine the order in which nodes are added to the `Workflow` that results from data model realization.
3. Null nodes are added to the graph.  These nodes are used to make it so all edges are part of an edge set, which makes the rest of this process easier.
4. Edges are chosen, and then nodes are chosen.
5. DFS to create entity instances for each chosen node, by using the `Generator` associated with each node (via `Node<T>.refine()`).
6. DFS to satisfy constraints for each chosen edge, by executing the lambda associated with each edge (via `Edge<U, V>.refine()`).

These steps are implemented in the `Preprocessor` and `Processor` classes.

## Data Loading

Data loading is done by implementors of the `DataLoader` interface (or with `DefaultDataLoader`), which is meant to orchestrate the loading and unloading of entity instances in a `Workflow`.

## Visualization

The data model realization process generates JSON that represents the graph that has been realized.  This JSON is injected into a JavaScript file (`graph_visualization.js`), which is then loaded by an HTML visualization (`graph_visualization.html`) of the user-specified data model and all of its realizations.

The visualizations are implemented using Cytoscape.js.